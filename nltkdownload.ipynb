{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'batch_reader'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3d94c23db3e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mbatch_reader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseq2seq_attention_decode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'batch_reader'"
     ]
    }
   ],
   "source": [
    "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"Trains a seq2seq model.\n",
    "WORK IN PROGRESS.\n",
    "Implement \"Abstractive Text Summarization using Sequence-to-sequence RNNS and\n",
    "Beyond.\"\n",
    "\"\"\"\n",
    "                                        \n",
    "import sys\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import batch_reader\n",
    "import data\n",
    "import seq2seq_attention_decode\n",
    "import seq2seq_attention_model\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_string('data_path',\n",
    "                           '', 'Path expression to tf.Example.')\n",
    "tf.app.flags.DEFINE_string('vocab_path',\n",
    "                           '', 'Path expression to text vocabulary file.')\n",
    "tf.app.flags.DEFINE_string('article_key', 'article',\n",
    "                           'tf.Example feature key for article.')\n",
    "tf.app.flags.DEFINE_string('abstract_key', 'headline',\n",
    "                           'tf.Example feature key for abstract.')\n",
    "tf.app.flags.DEFINE_string('log_root', '', 'Directory for model root.')\n",
    "tf.app.flags.DEFINE_string('train_dir', '', 'Directory for train.')\n",
    "tf.app.flags.DEFINE_string('eval_dir', '', 'Directory for eval.')\n",
    "tf.app.flags.DEFINE_string('decode_dir', '', 'Directory for decode summaries.')\n",
    "tf.app.flags.DEFINE_string('mode', 'train', 'train/eval/decode mode')\n",
    "tf.app.flags.DEFINE_integer('max_run_steps', 10000000,\n",
    "                            'Maximum number of run steps.')\n",
    "tf.app.flags.DEFINE_integer('max_article_sentences', 2,\n",
    "                            'Max number of first sentences to use from the '\n",
    "                            'article')\n",
    "tf.app.flags.DEFINE_integer('max_abstract_sentences', 100,\n",
    "                            'Max number of first sentences to use from the '\n",
    "                            'abstract')\n",
    "tf.app.flags.DEFINE_integer('beam_size', 4,\n",
    "                            'beam size for beam search decoding.')\n",
    "tf.app.flags.DEFINE_integer('eval_interval_secs', 60, 'How often to run eval.')\n",
    "tf.app.flags.DEFINE_integer('checkpoint_secs', 60, 'How often to checkpoint.')\n",
    "tf.app.flags.DEFINE_bool('use_bucketing', False,\n",
    "                         'Whether bucket articles of similar length.')\n",
    "tf.app.flags.DEFINE_bool('truncate_input', False,\n",
    "                         'Truncate inputs that are too long. If False, '\n",
    "                         'examples that are too long are discarded.')\n",
    "tf.app.flags.DEFINE_integer('num_gpus', 0, 'Number of gpus used.')\n",
    "tf.app.flags.DEFINE_integer('random_seed', 111, 'A seed value for randomness.')\n",
    "\n",
    "\n",
    "def _RunningAvgLoss(loss, running_avg_loss, summary_writer, step, decay=0.999):\n",
    "  \"\"\"Calculate the running average of losses.\"\"\"\n",
    "  if running_avg_loss == 0:\n",
    "    running_avg_loss = loss\n",
    "  else:\n",
    "    running_avg_loss = running_avg_loss * decay + (1 - decay) * loss\n",
    "  running_avg_loss = min(running_avg_loss, 12)\n",
    "  loss_sum = tf.Summary()\n",
    "  loss_sum.value.add(tag='running_avg_loss', simple_value=running_avg_loss)\n",
    "  summary_writer.add_summary(loss_sum, step)\n",
    "  sys.stdout.write('running_avg_loss: %f\\n' % running_avg_loss)\n",
    "  return running_avg_loss\n",
    "\n",
    "\n",
    "def _Train(model, data_batcher):\n",
    "  \"\"\"Runs model training.\"\"\"\n",
    "  with tf.device('/cpu:0'):\n",
    "    model.build_graph()\n",
    "    saver = tf.train.Saver()\n",
    "    # Train dir is different from log_root to avoid summary directory\n",
    "    # conflict with Supervisor.\n",
    "    summary_writer = tf.summary.FileWriter(FLAGS.train_dir)\n",
    "    sv = tf.train.Supervisor(logdir=FLAGS.log_root,\n",
    "                             is_chief=True,\n",
    "                             saver=saver,\n",
    "                             summary_op=None,\n",
    "                             save_summaries_secs=60,\n",
    "                             save_model_secs=FLAGS.checkpoint_secs,\n",
    "                             global_step=model.global_step)\n",
    "    sess = sv.prepare_or_wait_for_session(config=tf.ConfigProto(\n",
    "        allow_soft_placement=True))\n",
    "    running_avg_loss = 0\n",
    "    step = 0\n",
    "    while not sv.should_stop() and step < FLAGS.max_run_steps:\n",
    "      (article_batch, abstract_batch, targets, article_lens, abstract_lens,\n",
    "       loss_weights, _, _) = data_batcher.NextBatch()\n",
    "      (_, summaries, loss, train_step) = model.run_train_step(\n",
    "          sess, article_batch, abstract_batch, targets, article_lens,\n",
    "          abstract_lens, loss_weights)\n",
    "\n",
    "      summary_writer.add_summary(summaries, train_step)\n",
    "      running_avg_loss = _RunningAvgLoss(\n",
    "          running_avg_loss, loss, summary_writer, train_step)\n",
    "      step += 1\n",
    "      if step % 100 == 0:\n",
    "        summary_writer.flush()\n",
    "    sv.Stop()\n",
    "    return running_avg_loss\n",
    "\n",
    "\n",
    "def _Eval(model, data_batcher, vocab=None):\n",
    "  \"\"\"Runs model eval.\"\"\"\n",
    "  model.build_graph()\n",
    "  saver = tf.train.Saver()\n",
    "  summary_writer = tf.summary.FileWriter(FLAGS.eval_dir)\n",
    "  sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "  running_avg_loss = 0\n",
    "  step = 0\n",
    "  while True:\n",
    "    time.sleep(FLAGS.eval_interval_secs)\n",
    "    try:\n",
    "      ckpt_state = tf.train.get_checkpoint_state(FLAGS.log_root)\n",
    "    except tf.errors.OutOfRangeError as e:\n",
    "      tf.logging.error('Cannot restore checkpoint: %s', e)\n",
    "      continue\n",
    "\n",
    "    if not (ckpt_state and ckpt_state.model_checkpoint_path):\n",
    "      tf.logging.info('No model to eval yet at %s', FLAGS.train_dir)\n",
    "      continue\n",
    "\n",
    "    tf.logging.info('Loading checkpoint %s', ckpt_state.model_checkpoint_path)\n",
    "    saver.restore(sess, ckpt_state.model_checkpoint_path)\n",
    "\n",
    "    (article_batch, abstract_batch, targets, article_lens, abstract_lens,\n",
    "     loss_weights, _, _) = data_batcher.NextBatch()\n",
    "    (summaries, loss, train_step) = model.run_eval_step(\n",
    "        sess, article_batch, abstract_batch, targets, article_lens,\n",
    "        abstract_lens, loss_weights)\n",
    "    tf.logging.info(\n",
    "        'article:  %s',\n",
    "        ' '.join(data.Ids2Words(article_batch[0][:].tolist(), vocab)))\n",
    "    tf.logging.info(\n",
    "        'abstract: %s',\n",
    "        ' '.join(data.Ids2Words(abstract_batch[0][:].tolist(), vocab)))\n",
    "\n",
    "    summary_writer.add_summary(summaries, train_step)\n",
    "    running_avg_loss = _RunningAvgLoss(\n",
    "        running_avg_loss, loss, summary_writer, train_step)\n",
    "    if step % 100 == 0:\n",
    "      summary_writer.flush()\n",
    "\n",
    "\n",
    "def main(unused_argv):\n",
    "  vocab = data.Vocab(FLAGS.vocab_path, 1000000)\n",
    "  # Check for presence of required special tokens.\n",
    "  assert vocab.CheckVocab(data.PAD_TOKEN) > 0\n",
    "  assert vocab.CheckVocab(data.UNKNOWN_TOKEN) >= 0\n",
    "  assert vocab.CheckVocab(data.SENTENCE_START) > 0\n",
    "  assert vocab.CheckVocab(data.SENTENCE_END) > 0\n",
    "\n",
    "  batch_size = 4\n",
    "  if FLAGS.mode == 'decode':\n",
    "    batch_size = FLAGS.beam_size\n",
    "\n",
    "  hps = seq2seq_attention_model.HParams(\n",
    "      mode=FLAGS.mode,  # train, eval, decode\n",
    "      min_lr=0.01,  # min learning rate.\n",
    "      lr=0.15,  # learning rate\n",
    "      batch_size=batch_size,\n",
    "      enc_layers=4,\n",
    "      enc_timesteps=120,\n",
    "      dec_timesteps=30,\n",
    "      min_input_len=2,  # discard articles/summaries < than this\n",
    "      num_hidden=256,  # for rnn cell\n",
    "      emb_dim=128,  # If 0, don't use embedding\n",
    "      max_grad_norm=2,\n",
    "      num_softmax_samples=4096)  # If 0, no sampled softmax.\n",
    "\n",
    "  batcher = batch_reader.Batcher(\n",
    "      FLAGS.data_path, vocab, hps, FLAGS.article_key,\n",
    "      FLAGS.abstract_key, FLAGS.max_article_sentences,\n",
    "      FLAGS.max_abstract_sentences, bucketing=FLAGS.use_bucketing,\n",
    "      truncate_input=FLAGS.truncate_input)\n",
    "  tf.set_random_seed(FLAGS.random_seed)\n",
    "\n",
    "  if hps.mode == 'train':\n",
    "    model = seq2seq_attention_model.Seq2SeqAttentionModel(\n",
    "        hps, vocab, num_gpus=FLAGS.num_gpus)\n",
    "    _Train(model, batcher)\n",
    "  elif hps.mode == 'eval':\n",
    "    model = seq2seq_attention_model.Seq2SeqAttentionModel(\n",
    "        hps, vocab, num_gpus=FLAGS.num_gpus)\n",
    "    _Eval(model, batcher, vocab=vocab)\n",
    "  elif hps.mode == 'decode':\n",
    "    decode_mdl_hps = hps\n",
    "    # Only need to restore the 1st step and reuse it since\n",
    "    # we keep and feed in state for each step's output.\n",
    "    decode_mdl_hps = hps._replace(dec_timesteps=1)\n",
    "    model = seq2seq_attention_model.Seq2SeqAttentionModel(\n",
    "        decode_mdl_hps, vocab, num_gpus=FLAGS.num_gpus)\n",
    "    decoder = seq2seq_attention_decode.BSDecoder(model, batcher, hps, vocab)\n",
    "    decoder.DecodeLoop()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
